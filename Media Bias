{"cells":[{"cell_type":"markdown","metadata":{"id":"w5bfzue2jhHB"},"source":["Presentation: https://www.canva.com/design/DAGGY9eMVBI/X8ROg4LBTXMeyMNLFBjUEw/edit"]},{"cell_type":"markdown","metadata":{"id":"mc8uYmRli9HL"},"source":["#Data Collection"]},{"cell_type":"markdown","metadata":{"id":"N7khhzm9A0V7"},"source":["##Import Libraries"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8375,"status":"ok","timestamp":1718260968310,"user":{"displayName":"Jasmine Singh","userId":"12082738895923108038"},"user_tz":-480},"id":"9B2rmm0udBTe","outputId":"bd78d44c-b354-4537-db4d-c3318b1b79c9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.7.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n","Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.6.1)\n"]}],"source":["!pip install requests beautifulsoup4 pymongo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jct28d9S6OuA","outputId":"db4a7825-14d8-4b3c-f167-1effc58fe18d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.4)\n"]}],"source":["# Install the textblob module if it's not already installed\n","!pip install textblob\n","\n","# Import the TextBlob function from the textblob module\n","from textblob import TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPAh2gbfLg23"},"outputs":[],"source":["!pip install urljoin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1JGqpfvrTSy"},"outputs":[],"source":["%pip install nltk -U\n","%pip install spacy -U\n","%pip install gensim\n","%pip install pyldavis\n","%pip install pandas\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jgZAlVyFAV3h"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import json\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from urllib.parse import urljoin\n","import os\n","import nltk\n","import re\n","import string\n","import gensim\n","import numpy as np\n","import pandas as pd\n","\n","# for tokenization\n","from nltk.tokenize import word_tokenize\n","nltk.download(\"punkt\")\n","nltk.download('wordnet')\n","\n","# for stopword removal\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# for lemmatization and POS tagging\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","nltk.download('averaged_perceptron_tagger')\n","\n","# for LDA\n","from gensim import corpora\n","from gensim.models import LdaModel\n","from gensim.models.coherencemodel import CoherenceModel\n","\n","# for LDA evaluation\n","import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvisualize\n","from urllib.parse import urlparse, urlunparse"]},{"cell_type":"markdown","metadata":{"id":"cBwEd8qTdHwv"},"source":["##Parsing"]},{"cell_type":"markdown","metadata":{"id":"FZlz48AaGWHw"},"source":["###News"]},{"cell_type":"markdown","metadata":{"id":"qkXyndw6D9BS"},"source":["####US Elections:"]},{"cell_type":"markdown","metadata":{"id":"qf0rfYR_5obj"},"source":["#####CBS News (Left)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"La1LvPUr1TSP"},"outputs":[],"source":["df = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crANrtT5dJYz"},"outputs":[],"source":["usurl = \"https://www.cbsnews.com/feature/election-2024/\"\n","html_text = requests.get(usurl).text\n","articles = []\n","soup = BeautifulSoup(html_text, 'html.parser')\n","\n","# Loop through each article tag with class \"item--type-article\"\n","for article in soup.find_all('article', class_='item--type-article'):\n","    link_tag = article.find('a', class_='item__anchor')\n","    if link_tag:\n","        href = link_tag['href']\n","\n","        # Open each href link and extract the news article\n","        tx_text = requests.get(href).text\n","        soup_tx = BeautifulSoup(tx_text, 'html.parser')\n","        combined_text = \"\"\n","        for news in soup_tx.find_all('section', class_='content__body'):\n","            try:\n","                for p_tag in news.find_all('p'):\n","                    combined_text += p_tag.text.strip() + \" \"\n","            except:\n","                continue\n","\n","        title_tag = article.find('h4', class_='item__hed')\n","        if title_tag:\n","            title = title_tag.text.strip()\n","\n","        summary_tag = article.find('p', class_='item__dek')\n","        if summary_tag:\n","            summary = summary_tag.text.strip()\n","\n","        articles.append({\n","            'title': title,\n","            'href': href,\n","            'summary': summary,\n","            'article_text': combined_text,\n","            'news_source': 'CBSnews',\n","            'country': 'US',\n","        })\n","\n","\n","df = pd.DataFrame(articles)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtcNM_UZdPyt"},"outputs":[],"source":["cbs_json = json.dumps(articles)\n","\n","cbs_dict = json.loads(cbs_json)"]},{"cell_type":"markdown","metadata":{"id":"TT4IqKJ6jZ15"},"source":["#####Fox News (Right)"]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from urllib.parse import urlparse, urlunparse\n","\n","def scrape_articles(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    articles = []\n","\n","    # Find all relevant article elements\n","    for article in soup.find_all('article', class_='article'):\n","        try:\n","            title = article.find('h4', class_='title').text.strip() if article.find('h4', class_='title') else ''\n","            href = article.find('a', href=True)['href'].strip() if article.find('a', href=True) else ''\n","            if href and not (href.startswith('http://') or href.startswith('https://')):\n","                href = 'https://www.foxnews.com' + href\n","\n","            summary = article.find('p', class_='dek').text.strip() if article.find('p', class_='dek') else ''\n","\n","            # Fetch full article text\n","            if href:\n","                article_response = requests.get(href)\n","                article_soup = BeautifulSoup(article_response.content, 'html.parser')\n","                article_text_element = article_soup.find('div', class_='article-body')\n","                article_text = article_text_element.text.strip() if article_text_element else 'Not available'\n","            else:\n","                article_text = 'Not available'\n","\n","            articles.append({\n","                'title': title,\n","                'href': href,\n","                'summary': summary,\n","                'article_text': article_text,\n","                'news_source': 'Foxnews',\n","                'country': 'US'\n","            })\n","\n","        except Exception as e:\n","            print(f\"Error scraping article: {e}\")\n","\n","    return pd.DataFrame(articles)\n","\n","# Define URLs to scrape\n","urls = [\n","    \"https://www.foxnews.com/category/politics/elections/2023/primary-results\",\n","    \"https://www.foxnews.com/elections\",\n","    \"https://www.foxnews.com/category/us/congress\",\n","    \"https://www.foxnews.com/category/columns/elections-newsletter\",\n","    \"https://www.foxnews.com/category/person/donald-trump\",\n","    \"https://www.foxnews.com/politics\"\n","]\n","\n","# Scrape each URL and concatenate the results\n","dfs = [scrape_articles(url) for url in urls]\n","df = pd.concat(dfs, ignore_index=True)\n","\n","# Display the DataFrame\n","print(df)\n"],"metadata":{"id":"fCRbl3ZoUBFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df[df['article_text'] == 'Not available']\n","\n","df = df.drop(df[df['article_text'] == 'Not available'].index)\n","df"],"metadata":{"id":"SBV_vJ1DUj_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ExDvch4dzaD"},"source":["####Indian Elections"]},{"cell_type":"markdown","metadata":{"id":"WzWQuz6p5sHZ"},"source":["#####The Hindu (Left)"]},{"cell_type":"markdown","metadata":{"id":"xsNzvPeud5fG"},"source":["only able to read the first p tag for the article content or else i am getting this if i try to get the whole text from each website\n","\n","\n","```\n","# IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HE1X9616d1SF"},"outputs":[],"source":["url = \"https://www.thehindu.com/elections/\"\n","html_text = requests.get(url).text\n","soup = BeautifulSoup(html_text, 'html.parser')\n","\n","articles = []\n","\n","# Loop through each h3 tag with class \"title\"\n","for h3 in soup.find_all('h3', class_='title'):\n","    link_tag = h3.find('a')\n","    title = link_tag.text.strip()\n","    if link_tag:\n","        href = link_tag['href']\n","\n","        # Open each href link and extract the news article\n","        h_text = requests.get(href).text\n","        soup_h = BeautifulSoup(h_text, 'html.parser')\n","\n","        combinedhindu_text = \"\"\n","        # Find the div with the specific itemprop\n","        article_body_div = soup_h.find('div', itemprop='articleBody')\n","        if article_body_div:\n","          for p_tag in article_body_div.find_all('p'):\n","            combinedhindu_text += p_tag.text.strip() + \" \"\n","\n","    # Find the summary name\n","    subtitle_tag = soup_h.find('h2', class_='sub-title')\n","    if subtitle_tag:\n","            subtitle = subtitle_tag.text.strip()\n","    else:\n","            subtitle = 'None'\n","\n","    articles.append({\n","        'title': title,\n","        'href': href,\n","        'summary': subtitle,\n","        'article_text': combinedhindu_text,\n","        'news_source': 'TheHindu',\n","        'country': 'India',\n","    })\n","\n","df_hindu = pd.DataFrame(articles)\n","df = pd.concat([df, df_hindu], ignore_index=True)\n","df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jZSaBeLepBQ"},"outputs":[],"source":["thehindu_json = json.dumps(articles)\n","\n","thehindu_dict = json.loads(thehindu_json)\n"]},{"cell_type":"markdown","metadata":{"id":"F5_ylpOK544v"},"source":["#####Times of India (Centre right)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAisolv2zyoD"},"outputs":[],"source":["\n","toiurl = \"https://timesofindia.indiatimes.com/elections/lok-sabha-election\"\n","toihtml = requests.get(toiurl).text\n","toisoup = BeautifulSoup(toihtml, 'html.parser')\n","\n","articles = []\n","base_url = \"https://timesofindia.indiatimes.com\"\n","\n","# Scraping links from the main page\n","for news in toisoup.find_all('div', class_='iN5CR'):\n","    try:\n","        title_element = news.find('div', class_='WavNE')\n","        title = title_element.text.strip() if title_element else \"\"\n","\n","        href_element = news.find('a')\n","        href = href_element['href'].strip() if href_element else \"\"\n","\n","        if href and not href.startswith(\"http\"):\n","            href = base_url + href\n","\n","        if href:\n","            # Fetching content from each link\n","            response = requests.get(href)\n","            soup_h = BeautifulSoup(response.content, \"html.parser\")\n","\n","            # Extracting the headline from the specified div class\n","            headline_element = soup_h.find('div', class_='M1rHh undefined')\n","            headline = headline_element.text.strip() if headline_element else \"\"\n","\n","            # Extracting the article body from the specified div class\n","            article_body_element = soup_h.find('div', class_='_s30J clearfix')\n","            article_body = article_body_element.get_text(separator=\" \", strip=True) if article_body_element else \"\"\n","\n","            articles.append({\n","                'title': title,\n","                'href': href,\n","                'summary': headline,\n","                'article_text': article_body,\n","                'news_source': 'Times of India',\n","                'country': 'India',\n","            })\n","    except Exception as e:\n","        print(\"Error:\", e)\n","        continue\n","\n","# Print the extracted articles\n","for article in articles:\n","    print(f\"Title: {article['title']}\")\n","    print(f\"Link: {article['href']}\")\n","    print(f\"Headline: {article['summary']}\")\n","    print(f\"Article Text: {article['article_text']}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5tlwfZydTsFr"},"outputs":[],"source":["df_toi = pd.DataFrame(articles)\n","df = pd.concat([df, df_toi], ignore_index=True)\n","df"]},{"cell_type":"markdown","metadata":{"id":"lcLT2IzEUGgi"},"source":["#####FirstPost (Left)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A35zTqp_mKIM"},"outputs":[],"source":["data = []\n","\n","# Base URL of the website\n","base_url = \"https://www.firstpost.com/tag/2024-election-analysis/\"\n","\n","# Make a request to the base URL\n","response = requests.get(base_url)\n","soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","# Find all the links (this part assumes you have some way to get the initial 'links')\n","links = soup.find_all('a', href=True)\n","\n","for link in links:\n","    link_class = link.get('class')\n","    if link_class:\n","        links_with_class = soup.find_all('a', class_=link_class)\n","        for link_with_class in links_with_class:\n","            title_tag = link_with_class.find('p')\n","            if title_tag:\n","                title = title_tag.text.strip()\n","                href = link_with_class['href']\n","\n","                # Fetch the article content\n","                article_url = urljoin(base_url, href)\n","                article_response = requests.get(article_url)\n","                article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n","\n","                # Initialize variables\n","                summary = \"\"\n","                article_text = \"\"\n","\n","                # Extract the JSON-LD script tags\n","                script_tags = article_soup.find_all('script', type='application/ld+json')\n","                for script_tag in script_tags:\n","                    script_content = script_tag.string\n","                    json_data = json.loads(script_content)\n","\n","                    # Check if the JSON-LD type is WebPage for the summary\n","                    if json_data.get('@type') == 'WebPage':\n","                        summary = json_data.get('description', '')\n","\n","                    # Check if the JSON-LD type is NewsArticle for the article body\n","                    if json_data.get('@type') == 'NewsArticle':\n","                        article_text = json_data.get('articleBody', '')\n","\n","                # Append the data for this article to the list\n","                data.append({\n","                    'title': title,\n","                    'href': article_url,\n","                    'summary': summary,\n","                    'article_text': article_text,\n","                    'news_source': 'Firstpost',\n","                    'country': 'India',\n","\n","                })\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X47XeZAhmRD8"},"outputs":[],"source":["data_df = pd.DataFrame(data)\n","df = pd.concat([df, data_df], ignore_index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFhJXPt7odv9"},"outputs":[],"source":["df['news_source'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbIAJYeMNPXp"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"2VT5IZgk-7iH"},"source":["###Manifestos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6BLaOAr_iTg"},"outputs":[],"source":["#!pip install pdfquery PyPDF2\n","#import pdfquery\n","#import PyPDF2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGBjtXooi53k"},"outputs":[],"source":["# prompt: convert this to comment please\n","# def read_pdf(filename):\n","#     pdf = PyPDF2.PdfReader(open(filename, 'rb'))\n","#     text = \"\"\n","#     for page_number in range(len(pdf.pages)):\n","#         page = pdf.pages[page_number]\n","#         text += page.extract_text()\n","#     return text\n","# def extract_manifesto_text(filename):\n","#     pdf = PyPDF2.PdfReader(open(filename, 'rb'))\n","#     text_content = \"\"\n","#     for page_number in range(len(pd\n","\n","# def read_pdf(filename):\n","#     pdf = PyPDF2.PdfReader(open(filename, 'rb'))\n","#     text = \"\"\n","#     for page_number in range(len(pdf.pages)):\n","#         page = pdf.pages[page_number]\n","#         text += page.extract_text()\n","#     return text\n","# def extract_manifesto_text(filename):\n","#     pdf = PyPDF2.PdfReader(open(filename, 'rb'))\n","#     text_content = \"\"\n","#     for page_number in range(len(pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9h4cc_guiRY"},"outputs":[],"source":["# prompt: turn this to comment please\n","# manifesto_data = {\n","#     'Party': ['BJP', 'CPI', 'DMK', 'Congress'],\n","#     'Manifesto': [bjp_text, cpi_text, dmk_text, congress_text],\n","#     'Country': ['India', 'India', 'India', 'India'],\n","#     'Political Orientation': ['Right', 'Left', 'Left', 'Centre Left']\n","# }\n","# df_manifestos = pd.DataFrame(manifesto_data)\n","# # Print the DataFrame\n","# print(df_manifestos)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2Q42eMOejC5h"},"source":["#Pre-Processing"]},{"cell_type":"markdown","metadata":{"id":"NzOY2k7tDy1x"},"source":["Tokenization: Your tokenization process looks good, but you might want to consider using more advanced techniques for tokenization, such as handling contractions, special characters, and specific domain-related terms.\n","Sentiment Analysis: You're calculating sentiment scores for each country's news articles, which is great. However, you might want to further analyze the sentiment trends over time or compare sentiment scores across different news sources within each country."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mVG6WcH_0Fa"},"outputs":[],"source":["import spacy\n","from spacy.tokenizer import Tokenizer\n","import en_core_web_sm\n","\n","# Load the English tokenizer and language model\n","nlp = en_core_web_sm.load()\n","\n","# Initialize spaCy tokenizer\n","tokenizer = Tokenizer(nlp.vocab)\n","\n","# Define a function for preprocessing text using spaCy\n","def spacy_preprocess(text):\n","    # Process the text with spaCy\n","    doc = nlp(text)\n","    # Extract tokens\n","    tokens = [token.text for token in doc if not token.is_stop]\n","    return tokens\n","\n","# Function to preprocess text with enhanced tokenization\n","def enhanced_txt_preprocess_pipeline(text):\n","    # Standardize text to lowercase\n","    standard_txt = text.lower()\n","    # Remove multiple white spaces and line breaks\n","    clean_txt = re.sub(r'\\n', ' ', standard_txt)\n","    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n","    clean_txt = clean_txt.strip()\n","    # Handle contractions\n","    clean_txt = re.sub(r\"n't\", \" not\", clean_txt)\n","    clean_txt = re.sub(r\"'s\", \" is\", clean_txt)\n","    clean_txt = re.sub(r\"'m\", \" am\", clean_txt)\n","    # Tokenize text using spaCy\n","    tokens = spacy_preprocess(clean_txt)\n","    # Remove non-alphabetic tokens\n","    filtered_tokens_alpha = [word for word in tokens if word.isalpha()]\n","    # Load NLTK stopword list and add original stopwords\n","    stop_words = stopwords.words('english')\n","    stop_words.extend(['thee', 'thou', 'thy', 'ye', 'computer', 'gutenberg', 'http', 'chapter', 'mr', 'mrs', 'ms', 'dr'])\n","    # Remove stopwords\n","    filtered_tokens_final = [w for w in filtered_tokens_alpha if not w in stop_words]\n","    # Define lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    # Conduct POS tagging\n","    pos_tags = nltk.pos_tag(filtered_tokens_final)\n","    # Lemmatize word-tokens via assigned POS tags\n","    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n","    return lemma_tokens\n","\n","# Apply the enhanced preprocessing function to each article in the dataframe\n","df['processed_text'] = df['article_text'].apply(enhanced_txt_preprocess_pipeline)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8N73U3HXoyDR"},"outputs":[],"source":["india_df = df[df['country'] == 'India']\n","us_df= df[df['country'] == 'US']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wujFoeT63Rxk"},"outputs":[],"source":["# Initialize lemmatizer and stopwords\n","lmtzr = nltk.WordNetLemmatizer()\n","stop_words = set(stopwords.words(\"english\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAm6S-28r-yL"},"outputs":[],"source":["# load WordNet POS tags for lemmatization\n","def wordnet_pos_tags(treebank_tag):\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN\n","\n","# preprocessing function\n","def txt_preprocess_pipeline(text):\n","    # standardize text to lowercase\n","    standard_txt = text.lower()\n","    # remove multiple white spaces and line breaks\n","    clean_txt = re.sub(r'\\n', ' ', standard_txt)\n","    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n","    clean_txt = clean_txt.strip()\n","    # tokenize text\n","    tokens = word_tokenize(clean_txt)\n","    # remove non-alphabetic tokens\n","    filtered_tokens_alpha = [word for word in tokens if word.isalpha()]\n","    # load NLTK stopword list and add original stopwords\n","    stop_words = stopwords.words('english')\n","    stop_words.extend(['thee', 'thou', 'thy', 'ye', 'computer', 'gutenberg', 'http', 'chapter', 'mr', 'mrs', 'ms', 'dr'])\n","    # remove stopwords\n","    filtered_tokens_final = [w for w in filtered_tokens_alpha if not w in stop_words]\n","    # define lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    # conduct POS tagging\n","    pos_tags = nltk.pos_tag(filtered_tokens_final)\n","    # lemmatize word-tokens via assigned POS tags\n","    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n","    return lemma_tokens\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flX8w4_FvLNi"},"outputs":[],"source":["# Apply the preprocessing function to each article in the dataframe\n","india_df['processed_text'] = india_df['article_text'].apply(txt_preprocess_pipeline)\n","\n","# View the processed text\n","india_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgwGpTfPvatz"},"outputs":[],"source":["# Apply the preprocessing function to each article in the dataframe\n","us_df['processed_text'] = us_df['article_text'].apply(txt_preprocess_pipeline)\n","\n","# View the processed text\n","us_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12LGwdy9viiK"},"outputs":[],"source":["us_df['processed_text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCJeeBnOEycY"},"outputs":[],"source":["india_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfFsKx0xBODR"},"outputs":[],"source":["india_df['news_label'] = india_df['news_source'].apply(lambda x: 'Left' if x == 'Firstpost' else 'Centre Right' if x == 'TheHindu' else 'Left' if x == 'Times of India' else None)\n","india_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UK0B58fBXMVH"},"outputs":[],"source":["# prompt: can you help me export df dataset\n","\n","df.to_csv('df.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1fi1oDUCBNZ"},"outputs":[],"source":["us_df['news_label'] = us_df['news_source'].apply(lambda x: 'Left' if x == 'CBSnews' else 'Right' if x == 'Foxnews'  else None)\n"]},{"cell_type":"markdown","metadata":{"id":"Q-8vbb67jwUI"},"source":["#Topic Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHiDBfdk1ZQg"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","# Function to get topic names\n","def get_topic_names(model, feature_names, no_top_words):\n","    topic_names = []\n","    for topic_idx, topic in enumerate(model.components_):\n","        topic_name = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n","        topic_names.append(topic_name)\n","    return topic_names\n","\n","# Function to perform topic modeling and get distributions\n","def perform_topic_modeling(df, n_topics=10, no_top_words=5):\n","    vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n","    dtm = vectorizer.fit_transform(df['processed_text'].apply(lambda x: ' '.join(x)))\n","    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n","    lda.fit(dtm)\n","\n","    feature_names = vectorizer.get_feature_names_out()\n","    topic_names = get_topic_names(lda, feature_names, no_top_words)\n","\n","    df['topic_distribution'] = list(lda.transform(dtm))\n","    topic_dist = df.groupby('news_source')['topic_distribution'].apply(np.mean)\n","    topic_df = pd.DataFrame(topic_dist.tolist(), index=topic_dist.index, columns=topic_names)\n","\n","    return topic_df, topic_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjTKaur90qbQ"},"outputs":[],"source":["# Plotting function\n","def plot_topic_distribution(topic_df, country_name):\n","    topic_df.plot(kind='bar', stacked=True, figsize=(10, 7), colormap='tab20')\n","    plt.title(f'Topic Distribution by News Source in {country_name}')\n","    plt.xlabel('News Source')\n","    plt.ylabel('Proportion')\n","    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SgKimMKtyQYp"},"source":["####India"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB0cVZFbzB0g"},"outputs":[],"source":["# Perform topic modeling separately for India and US\n","india_topic_df, india_topic_names = perform_topic_modeling(india_df, n_topics=10, no_top_words=5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eazajBtn0WlQ"},"outputs":[],"source":["# Plot the topic distributions\n","plot_topic_distribution(india_topic_df, 'India')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bz5SsYBQ2N3g"},"outputs":[],"source":["# Display the topic names\n","print(\"India Topics:\")\n","for idx, topic in enumerate(india_topic_names):\n","    print(f\"Topic {idx+1}: {topic}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQqp2F9P1G_f"},"outputs":[],"source":["us_topic_df, us_topic_names = perform_topic_modeling(us_df, n_topics=10, no_top_words=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SaQQO4y2MDG"},"outputs":[],"source":["plot_topic_distribution(us_topic_df, 'US')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2r0NvI732Qne"},"outputs":[],"source":["print(\"\\nUS Topics:\")\n","for idx, topic in enumerate(us_topic_names):\n","    print(f\"Topic {idx+1}: {topic}\")"]},{"cell_type":"markdown","metadata":{"id":"VwwuOYEt3BxH"},"source":["#Sentiment Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6uz1dW22ifF"},"outputs":[],"source":["pip install chart_studio\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-Zw2H047Kmb"},"outputs":[],"source":["def calculate_sentiment_score(text):\n","    blob = TextBlob(text)\n","    return blob.sentiment.polarity\n","\n","# Apply the function to each row in the DataFrame and store the result in a new column\n","df['article_sentiment_score'] = df['article_text'].apply(calculate_sentiment_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BjSgeoov1ndu"},"outputs":[],"source":["import plotly.express as px\n","import pandas as pd\n","\n","# Assuming df is your DataFrame with the required data\n","# Convert the DataFrame to long format for Plotly\n","df_long = pd.melt(df, id_vars=['country', 'news_source'], value_vars=['article_sentiment_score'], var_name='Sentiment', value_name='Score')\n","\n","# Create an interactive box plot using Plotly Express\n","fig = px.box(df_long, x='country', y='Score', color='news_source', title='Sentiment Scores by News Source and Country',\n","             labels={'Score': 'Sentiment Score', 'country': 'Country', 'news_source': 'News Source'},\n","             category_orders={'country': sorted(df_long['country'].unique())},\n","             width=1200, height=800)\n","\n","# Customize the appearance of the plot\n","fig.update_layout(title_x=0.5, xaxis_tickangle=-45, legend_title='News Source')\n","\n","# Show the interactive plot\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgTCbP3--LeB"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"ZG9Oiwor5Pc0"},"source":["#Text Modeling with Sentiment Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1zh5J7l9M0i"},"outputs":[],"source":["# Install vaderSentiment library if not already installed\n","!pip install vaderSentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3X7Q87fcvAcf"},"outputs":[],"source":["# Import necessary libraries\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import pandas as pd\n","\n","# Initialize the VADER sentiment analyzer\n","analyzer = SentimentIntensityAnalyzer()\n","\n","# Function to calculate sentiment score using VADER\n","def calculate_vader_sentiment(text):\n","    sentiment_score = analyzer.polarity_scores(text)\n","    return sentiment_score['compound']  # Using compound score as an overall sentiment score\n","\n","# Add a new column for VADER sentiment scores to the DataFrame\n","df['vader_sentiment_score'] = df['article_text'].apply(calculate_vader_sentiment)\n","\n","# Vectorize the text data\n","vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","dtm = vectorizer.fit_transform(df['article_text'])\n","\n","# Fit the LDA model\n","lda = LatentDirichletAllocation(n_components=10, random_state=42)\n","lda.fit(dtm)\n","\n","# Get the topic distribution for each document\n","topic_distribution = lda.transform(dtm)\n","\n","# Add the topic distribution to the DataFrame\n","for i in range(lda.n_components):\n","    df[f'topic_{i}_distribution'] = topic_distribution[:, i]\n","\n","# Group by news source and aggregate sentiment scores for each topic\n","topic_sentiment_by_source = df.groupby('news_source')[['topic_0_distribution', 'topic_1_distribution', 'topic_2_distribution', 'topic_3_distribution', 'topic_4_distribution', 'topic_5_distribution', 'topic_6_distribution', 'topic_7_distribution', 'topic_8_distribution', 'topic_9_distribution']].mean()\n","\n","# Reset index to make it a DataFrame\n","topic_sentiment_by_source.reset_index(inplace=True)\n","\n","# Display the aggregated sentiment scores for each topic by news source\n","print(topic_sentiment_by_source)"]},{"cell_type":"markdown","metadata":{"id":"3eTzXf64uZQz"},"source":["Liwc: https://lit.eecs.umich.edu/geoliwc/liwc_dictionary.html\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YkoFuFIjvYjx"},"source":["Tfidf for two different labeled datasets\n","calculate the log odds of each token"]},{"cell_type":"markdown","metadata":{"id":"CAKz79aYwtnI"},"source":["Lexicons for media analysis for comprehensive anaylsis such as objectivity, opinions..."]},{"cell_type":"markdown","metadata":{"id":"zLkCybrixamy"},"source":["References to the candidates and their sentiment scores in each media\n","whcih political side is using more negativity.\n","(Domination of media: by anaylzying the number of times the candidate appears)\n","(negaitivity is constant or increases)\n","how is the negativity for opposition compared to the their own candidate\n","\n","Right wing wont say alot of good things about the right but alot of bad things about left( USE this as )"]},{"cell_type":"markdown","metadata":{"id":"zFl7ESAbzMVm"},"source":["List of list then put it into dataframe for sentiment analysis"]},{"cell_type":"markdown","metadata":{"id":"iaFCm3BrzPZ5"},"source":["drop the sentences taht dont really have the candidate or party names"]},{"cell_type":"markdown","metadata":{"id":"TZyw3K-czbG3"},"source":["trump score, biden score"]},{"cell_type":"code","source":["topic_sentiment_by_source"],"metadata":{"id":"4mSmQ6exmEyG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfdZQwATz7BZ"},"source":["chose two paper for both countries,\n","-"]},{"cell_type":"markdown","metadata":{"id":"LGOsWoQaxhGJ"},"source":["###Analyze Media Bias"]},{"cell_type":"markdown","metadata":{"id":"-YQ4tJHKxjVy"},"source":["####Compare Sentiment Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix1ZlXBKxlSB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"f5YYAPYWxmAB"},"source":["####Compare Mention Frequency"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apZlvhZZxo6p"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"2EbIvLDaxpRh"},"source":["####Negativity towards each candidate"]},{"cell_type":"markdown","metadata":{"id":"cJVJwvmHxxna"},"source":["#Genreate Scores for each candiate"]},{"cell_type":"markdown","metadata":{"id":"PkSHA2XbxpOZ"},"source":[]}],"metadata":{"colab":{"collapsed_sections":["N7khhzm9A0V7","qf0rfYR_5obj","2VT5IZgk-7iH"],"provenance":[{"file_id":"1aTNU90WxW6k_jw9yGihFcvIylU_2mV4W","timestamp":1716787622526}],"authorship_tag":"ABX9TyPNweeJ4aXrolOTdDn2EmrF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}